{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies \n",
    "\n",
    "! pip install spacy wikipedia\n",
    "! python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math \n",
    "\n",
    "import wikipedia\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can do it for many movies \n",
    "pages = [\n",
    "  \"The Tigger Movie 2000\", \"Dinosaur 2000 Movie\", \"The Emperor's New Groove 2000\", \"Recess School's Out\", \"Atlantis: The Lost Empire\", \"Monsters, Inc\", \"Return to Never Land\", \"Lilo & Stitch\", \"Treasure Planet 2002\", \"The Jungle Book 2 2003\", \"Piglet's Big Movie 2003\", \"Finding Nemo 2003\", \"Brother Bear 2003\", \"Teacher's Pet 2004\", \"Home on the Range 2004\", \"The Incredibles 2004\", \"Pooh's Heffalump Movie 2005\", \"Valiant 2005\", \"Chicken Little 2005\", \"The Wild 2006\", \"Cars 2006\", \"The Nightmare Before Christmas\", \"Meet the Robinsons 2007\", \"Ratatouille 2007\", \"WALL-E 2008\", \"Roadside Romeo 2008\", \"Bolt 2008\", \"Up 2009\",\n",
    "  \"A Christmas Carol 2009\", \"The Princess and the Frog 2009\", \"Toy Story\", \"Toy Story 2\", \"Toy Story 3\", \"Tangled\", \"Mars Needs Moms\", \"Cars 2\", \"Winnie the Pooh 2011\", \"Arjun: The Warrior Prince\", \"Brave 2012\", \"Frankenweenie\", \"Wreck-It Ralph\", \"Monsters University\", \"Planes film\", \"Frozen 2013\", \"Planes: Fire & Rescue\", \"Big Hero 6\", \"Inside Out\", \"The Good Dinosaur\", \"Zootopia\", \"Finding Dory\", \"Moana 2016\", \"Cars 3\", \"Coco 2017 film\", \"Incredibles 2\",\n",
    "  \"Shrek\", \"Shrek 2\", \"Shrek 3\", \"Antz\", \"A bugs life\", \"Bee movie\", \"Madagascar 2005 film\", \"Madagascar 2\", \"Kung fu panda\", \"Kung fu panda 2\",\"Kung fu panda 3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [\n",
    "    \"Toy Story\", \"Toy Story 2\", \"Toy Story 3\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing page Toy Story ...\n",
      " Processing page Toy Story 2 ...\n",
      " Processing page Toy Story 3 ...\n",
      "All pages processed\n",
      "vocabulary size : 2561 \n",
      "Toy Story\n",
      "Most frequent: [u'toy', u'film', u'story', u'disney', u'the', u'release', u'buzz', u'lasseter', u'pixar', u'woody', u'character', u'animation', u'feature', u'andy', u'katzenberg', u'in', u'computer', u'it', u'time', u'new']\n",
      "Higher TF-IDF: [u'sid', u'figure', u'final', u'whedon', u'buddy', u'adult', u'dummy', u'joss', u'desire', u'attraction', u'disneyland', u'guest', u'ride', u'render', u'test', u'emerge', u'real', u'industry', u'pull', u'door']\n",
      "Toy Story 2\n",
      "Most frequent: [u'toy', u'film', u'story', u'the', u'woody', u'release', u'lasseter', u'pixar', u'buzz', u'original', u'andy', u'sequel', u'disney', u'work', u'feature', u'production', u'al', u'new', u'award', u'animated']\n",
      "Higher TF-IDF: [u'stinky', u'roundup', u'museum', u'roth', u'entire', u'she', u'dust', u'forget', u'apartment', u'squeaker', u'airport', u'geri', u'asset', u'mcwhiggin', u'utility', u'cause', u'nominated', u'belong', u'emily', u'far']\n",
      "Toy Story 3\n",
      "Most frequent: [u'film', u'toy', u'story', u'the', u'release', u'gross', u'disney', u'pixar', u'highest', u'woody', u'andy', u'second', u'feature', u'animated', u'it', u'lotso', u'million', u'buzz', u'in', u'animate']\n",
      "Higher TF-IDF: [u'lotso', u'sunnyside', u'up', u'die', u'fourth', u'uk', u'bear', u'campaign', u'mexico', u'arndt', u'men', u'minions', u'sneak', u'spanish', u'dumpster', u'shrek', u'recording', u'malta', u'ireland', u'cooley']\n"
     ]
    }
   ],
   "source": [
    "def valid_token(tk):\n",
    "    is_valid = tk.is_alpha\n",
    "    return is_valid and not tk.is_stop\n",
    "\n",
    "def get_lemma(tk):\n",
    "    if tk.pos_ == 'PRON' or tk.lemma_ == '-PRON-':\n",
    "        return tk.text.lower()\n",
    "    return tk.lemma_.lower()\n",
    "\n",
    "def read_wikipedia_page(page_name):\n",
    "    page = wikipedia.page(page_name)\n",
    "    content = page.content\n",
    "    return content\n",
    "\n",
    "def tokenize_page(page_name):\n",
    "    text = read_wikipedia_page(page_name)\n",
    "    return [\n",
    "        get_lemma(t)\n",
    "        for t in nlp(text)\n",
    "        if valid_token(t)\n",
    "    ]\n",
    "\n",
    "vocabulary = set()\n",
    "idf_counter = Counter()\n",
    "\n",
    "for page in pages:\n",
    "    print(' Processing page {} ...'.format(page))\n",
    "    page_words = set(tokenize_page(page))\n",
    "    vocabulary = vocabulary | page_words\n",
    "    idf_counter.update(page_words)\n",
    "    \n",
    "print('All pages processed')\n",
    "\n",
    "idf = {\n",
    "    word: math.log(len(pages)/df, 2)\n",
    "    for word, df in idf_counter.items()\n",
    "}\n",
    "\n",
    "print('vocabulary size : {} '.format(len(vocabulary)))\n",
    "\n",
    "def analyze_page(target_page):\n",
    "    target_words = tokenize_page(target_page)\n",
    "    tfidf = {\n",
    "        word : (1 + math.log(_tf,2)) * idf[word]\n",
    "        for word, _tf in Counter(target_words).items()\n",
    "    }\n",
    "    \n",
    "    num_words = 20 \n",
    "    most_frequent = [\n",
    "        w for (w, _) in Counter(target_words).most_common(num_words)\n",
    "    ]\n",
    "    \n",
    "    sorted_tfidf = [\n",
    "        w for (w, _) in sorted(tfidf.items(), key=lambda kv: kv[1], reverse = True)\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    print(target_page)\n",
    "    print('Most frequent: {}'.format(most_frequent))\n",
    "    print('Higher TF-IDF: {}'.format(sorted_tfidf[:num_words]))\n",
    "\n",
    "analyze_page(\"Toy Story\")\n",
    "analyze_page(\"Toy Story 2\")\n",
    "analyze_page(\"Toy Story 3\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source  : https://building.lang.ai/wtf-is-tf-idf-5c5b86ee7331\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf",
   "language": "python",
   "name": "keras_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
